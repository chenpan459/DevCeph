# Ceph 分布式存储系统框架详解

## 📋 目录
1. [Ceph 整体架构](#一ceph-整体架构)
2. [核心组件详解](#二核心组件详解)
3. [三种存储接口](#三三种存储接口)
4. [客户端与服务端交互流程](#四客户端与服务端交互流程)
5. [CRUSH 算法与数据分布](#五crush-算法与数据分布)
6. [集群启动与初始化](#六集群启动与初始化)
7. [故障处理与自愈](#七故障处理与自愈)

---

## 一、Ceph 整体架构

### 1.1 架构分层视图

```
┌─────────────────────────────────────────────────────────────────────┐
│                          应用层                                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐           │
│  │  虚拟机  │  │  容器    │  │  数据库  │  │  备份    │           │
│  │ (QEMU)   │  │(Docker/K8s)│  │ (MySQL)  │  │  系统    │           │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘           │
└───────┼─────────────┼─────────────┼─────────────┼──────────────────┘
        │             │             │             │
┌───────┼─────────────┼─────────────┼─────────────┼──────────────────┐
│       │      客户端接口层          │             │                   │
│       ▼             ▼             ▼             ▼                   │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐               │
│  │   RBD   │  │ CephFS  │  │  RGW    │  │librados │               │
│  │ (块存储) │  │(文件系统)│  │(对象存储)│  │ (原生)  │               │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘               │
│       │            │            │             │                     │
│       └────────────┴────────────┴─────────────┘                     │
│                           │                                         │
│                           ▼                                         │
│              ┌────────────────────────┐                             │
│              │      librados          │                             │
│              │   (Ceph 客户端库)      │                             │
│              └────────────┬───────────┘                             │
│                           │                                         │
│                           │ RADOS Protocol                          │
│                           │ (TCP/IP)                                │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│              Ceph 集群（服务端）                                     │
│                           │                                         │
│              ┌────────────▼───────────┐                             │
│              │   RADOS (可靠自主分布式│                             │
│              │      对象存储系统)      │                             │
│              └────────────┬───────────┘                             │
│                           │                                         │
│       ┌───────────────────┼───────────────────┐                    │
│       │                   │                   │                    │
│       ▼                   ▼                   ▼                    │
│  ┌─────────┐        ┌─────────┐        ┌─────────┐               │
│  │ Monitor │        │   OSD   │        │   MDS   │               │
│  │  (Mon)  │        │ (数据)  │        │(元数据) │               │
│  │         │        │         │        │         │               │
│  │ • 集群  │        │ • 存储  │        │ • CephFS│               │
│  │   状态  │        │   数据  │        │   专用  │               │
│  │ • OSDMap│        │ • 复制  │        │         │               │
│  │ • PGMap │        │ • 恢复  │        │         │               │
│  └─────────┘        └─────────┘        └─────────┘               │
│                           │                                         │
│                           ▼                                         │
│              ┌────────────────────────┐                             │
│              │     BlueStore          │                             │
│              │   (存储引擎)           │                             │
│              └────────────┬───────────┘                             │
│                           │                                         │
│                           ▼                                         │
│              ┌────────────────────────┐                             │
│              │  物理存储设备           │                             │
│              │  (SSD/HDD/NVMe)        │                             │
│              └────────────────────────┘                             │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.2 核心设计理念

**1. 去中心化架构**
- ❌ 无单点故障（No Single Point of Failure）
- ✅ 客户端直接与 OSD 通信
- ✅ CRUSH 算法计算数据位置（无需查询元数据服务器）

**2. 自我管理**
- ✅ 智能 OSD：自动处理复制、恢复、重平衡
- ✅ 自我修复：检测并修复数据损坏
- ✅ 自动扩展：新节点加入自动重平衡

**3. 统一存储**
- ✅ 一个集群提供三种接口：块、文件、对象
- ✅ 底层统一的 RADOS 对象存储
- ✅ 简化运维

---

## 二、核心组件详解

### 2.1 Monitor (监控器)

```
┌───────────────────────────────────────────────────────────┐
│                    Monitor 集群                            │
│                                                            │
│     Mon A           Mon B           Mon C                 │
│    (Leader)        (Peon)          (Peon)                 │
│       │               │               │                    │
│       └───────┬───────┴───────┬───────┘                    │
│               │               │                            │
│         Paxos 一致性协议      │                            │
│                               │                            │
│  维护的集群地图：                                          │
│  ┌─────────────────────────────────────────────────┐     │
│  │ 1. Monitor Map：Monitor 集群拓扑                │     │
│  │ 2. OSD Map：OSD 状态和分布                      │     │
│  │ 3. PG Map：PG 统计信息                          │     │
│  │ 4. CRUSH Map：数据分布规则                      │     │
│  │ 5. MDS Map：MDS 状态（CephFS 使用）            │     │
│  └─────────────────────────────────────────────────┘     │
└───────────────────────────────────────────────────────────┘
```

**职责**：
- 🔹 维护集群状态的主副本
- 🔹 通过 Paxos 保证一致性
- 🔹 分发集群地图给客户端和 OSD
- 🔹 处理认证（cephx）
- 🔹 接收 OSD 状态报告

**部署建议**：
- 奇数部署：3 个或 5 个 Monitor
- 容忍故障数 = (n-1)/2

### 2.2 OSD (Object Storage Daemon)

```
┌───────────────────────────────────────────────────────────┐
│                    OSD 守护进程                            │
│                                                            │
│  ┌─────────────────────────────────────────────────┐     │
│  │          OSD Daemon (ceph-osd)                  │     │
│  │                                                  │     │
│  │  管理的 PG：                                     │     │
│  │  ┌────────┐  ┌────────┐  ┌────────┐           │     │
│  │  │ PG 1.a │  │ PG 2.b │  │ PG 3.c │  ...     │     │
│  │  └────────┘  └────────┘  └────────┘           │     │
│  │                                                  │     │
│  │  每个 PG 包含多个对象                           │     │
│  │  ┌──────────────────────────────────┐          │     │
│  │  │ PG 1.a:                          │          │     │
│  │  │   - object_1                     │          │     │
│  │  │   - object_2                     │          │     │
│  │  │   - object_3                     │          │     │
│  │  │   - ...                          │          │     │
│  │  └──────────────────────────────────┘          │     │
│  └─────────────────┬───────────────────────────────┘     │
│                    │                                       │
│                    ▼                                       │
│  ┌─────────────────────────────────────────────────┐     │
│  │            BlueStore                             │     │
│  │          (存储后端)                              │     │
│  │                                                  │     │
│  │  ┌──────────────┐  ┌──────────────┐           │     │
│  │  │   RocksDB    │  │  Block Device│           │     │
│  │  │   (元数据)   │  │   (数据)     │           │     │
│  │  └──────────────┘  └──────────────┘           │     │
│  └─────────────────────────────────────────────────┘     │
│                    │                                       │
│                    ▼                                       │
│        ┌────────────────────────┐                         │
│        │   物理磁盘 (/dev/sdb)  │                         │
│        └────────────────────────┘                         │
└───────────────────────────────────────────────────────────┘
```

**职责**：
- 🔹 存储实际数据
- 🔹 处理读写请求
- 🔹 数据复制（Primary → Replica）
- 🔹 数据恢复
- 🔹 心跳检测
- 🔹 向 Monitor 报告状态

**关键概念**：
- **PG (Placement Group)**：对象的逻辑分组
- **Acting Set**：负责某个 PG 的 OSD 集合
- **Primary OSD**：处理客户端请求的主 OSD

### 2.3 MDS (Metadata Server)

```
┌───────────────────────────────────────────────────────────┐
│                  MDS (仅 CephFS 使用)                      │
│                                                            │
│  Active MDS                Standby MDS                    │
│  ┌──────────────┐          ┌──────────────┐              │
│  │  处理元数据  │          │  热备         │              │
│  │  请求        │          │  随时接管     │              │
│  └──────────────┘          └──────────────┘              │
│                                                            │
│  管理的元数据：                                            │
│  • 文件名 → inode 映射                                    │
│  • 目录结构                                               │
│  • 权限和所有权                                           │
│  • 时间戳（mtime, ctime, atime）                         │
│  • 文件大小                                               │
│                                                            │
│  数据本身存储在 OSD！                                      │
└───────────────────────────────────────────────────────────┘
```

**职责**（仅 CephFS）：
- 🔹 管理文件系统元数据
- 🔹 目录树遍历
- 🔹 权限检查
- 🔹 动态子树分区（负载均衡）

---

## 三、三种存储接口

### 3.1 RBD (RADOS Block Device) - 块存储

```
┌────────────────────────────────────────────────┐
│            RBD 架构                             │
│                                                │
│  虚拟机/容器                                    │
│      │                                         │
│      ▼                                         │
│  /dev/rbd0 (4TB 虚拟块设备)                   │
│      │                                         │
│      ▼                                         │
│  ┌──────────────────────────────────────┐     │
│  │  librbd (RBD 客户端库)               │     │
│  │  • 条带化：将块设备分割成 4MB 对象   │     │
│  │  • 快照：增量快照                     │     │
│  │  • 克隆：写时复制                     │     │
│  └──────────────┬───────────────────────┘     │
│                 │                              │
│                 ▼                              │
│  ┌──────────────────────────────────────┐     │
│  │      librados                         │     │
│  │  将 RBD 对象映射到 RADOS 对象        │     │
│  └──────────────┬───────────────────────┘     │
│                 │                              │
│                 ▼                              │
│  ┌──────────────────────────────────────┐     │
│  │    RADOS (OSD 集群)                  │     │
│  │                                       │     │
│  │  rbd_data.12345.0000000000000000     │     │
│  │  rbd_data.12345.0000000000000001     │     │
│  │  rbd_data.12345.0000000000000002     │     │
│  │  ...                                  │     │
│  │  (每个对象 4MB)                       │     │
│  └───────────────────────────────────────┘     │
└────────────────────────────────────────────────┘
```

**使用场景**：
- ✅ 虚拟机磁盘（OpenStack Cinder、QEMU/KVM）
- ✅ 容器持久化存储（Kubernetes CSI）
- ✅ 数据库存储（MySQL、PostgreSQL）

**特性**：
- 精简配置（Thin Provisioning）
- 快照和克隆
- 支持内核模块（krbd）和用户态（librbd）

### 3.2 CephFS (Ceph File System) - 文件系统

```
┌────────────────────────────────────────────────┐
│           CephFS 架构                           │
│                                                │
│  应用程序                                       │
│      │                                         │
│      ▼                                         │
│  /mnt/cephfs/myfile.txt                       │
│      │                                         │
│      ▼                                         │
│  ┌──────────────────────────────────────┐     │
│  │  CephFS Client                        │     │
│  │  (内核模块 或 ceph-fuse)              │     │
│  └──────────┬───────────────────────────┘     │
│             │                                  │
│    ┌────────┼────────┐                        │
│    │ 元数据 │ 数据   │                        │
│    ▼        ▼        │                        │
│  ┌────┐  ┌──────────────────────────┐        │
│  │MDS │  │  librados                 │        │
│  │    │  │  (直接访问 OSD)           │        │
│  └────┘  └──────────┬───────────────┘        │
│                     │                         │
│                     ▼                         │
│          ┌────────────────────┐              │
│          │   OSD 集群          │              │
│          │  存储文件数据       │              │
│          └────────────────────┘              │
└────────────────────────────────────────────────┘
```

**使用场景**：
- ✅ 共享文件系统（多客户端读写）
- ✅ 大数据分析（Hadoop、Spark）
- ✅ HPC（高性能计算）

**特性**：
- POSIX 兼容
- 支持多个活跃 MDS（扩展性）
- 目录快照
- 配额管理

### 3.3 RGW (RADOS Gateway) - 对象存储

```
┌────────────────────────────────────────────────┐
│            RGW 架构                             │
│                                                │
│  S3/Swift 客户端                               │
│      │                                         │
│      ▼ (HTTP/HTTPS)                           │
│  ┌──────────────────────────────────────┐     │
│  │  radosgw (RGW 守护进程)              │     │
│  │  • RESTful API                        │     │
│  │  • S3 兼容接口                        │     │
│  │  • Swift 兼容接口                     │     │
│  │  • 用户管理                           │     │
│  │  • 桶（Bucket）管理                   │     │
│  └──────────────┬───────────────────────┘     │
│                 │                              │
│                 ▼                              │
│  ┌──────────────────────────────────────┐     │
│  │      librados                         │     │
│  │  • 对象元数据 → .rgw.meta             │     │
│  │  • 对象数据 → .rgw.buckets.data      │     │
│  │  • 用户信息 → .rgw.root               │     │
│  └──────────────┬───────────────────────┘     │
│                 │                              │
│                 ▼                              │
│  ┌──────────────────────────────────────┐     │
│  │       OSD 集群                        │     │
│  └───────────────────────────────────────┘     │
└────────────────────────────────────────────────┘
```

**使用场景**：
- ✅ 云存储服务（类似 AWS S3）
- ✅ 静态网站托管
- ✅ 备份和归档
- ✅ 多媒体内容分发

**特性**：
- S3 和 Swift API 兼容
- 多租户支持
- 跨区域复制
- 生命周期管理

---

## 四、客户端与服务端交互流程

### 4.1 RBD 写入流程（详细版）

```
第一步：客户端准备
═════════════════════════════════════════════════════════════

虚拟机/应用
   │
   │ write(fd, buffer, 4096)  // 写入 4KB 数据
   ↓
┌─────────────────────────────────────────────────┐
│  操作系统 (Linux Kernel)                         │
│  • VFS 层                                        │
│  • Block 层                                      │
└────────────────┬────────────────────────────────┘
                 │
                 ↓
┌─────────────────────────────────────────────────┐
│  /dev/rbd0 (内核 RBD 驱动 或 librbd)            │
│                                                  │
│  1. 确定写入位置：                               │
│     - 偏移量：1024 MB                           │
│     - 大小：4 KB                                │
│                                                  │
│  2. 计算对象 ID：                                │
│     - RBD image: "vm-disk-001"                  │
│     - Object 条带大小: 4 MB                     │
│     - 对象编号 = 1024MB / 4MB = 256            │
│     - 对象名: "rbd_data.12345.0000000100"      │
│     - 对象内偏移: 0                             │
└────────────────┬────────────────────────────────┘
                 │
                 ↓

第二步：定位数据位置
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  librados (Ceph 客户端库)                       │
│                                                  │
│  3. 查询集群地图（从 Monitor 获取）：            │
│     - OSDMap epoch: 1523                        │
│     - Pool: "rbd" (id=2)                        │
│     - PG 数量: 1024                             │
│                                                  │
│  4. 计算 PG：                                    │
│     hash("rbd_data.12345.0000000100")           │
│       = 0x7a3f2b1c                              │
│     PG = hash % pg_num                          │
│        = 0x7a3f2b1c % 1024                      │
│        = 452                                    │
│     完整 PG ID: 2.1c4 (pool.pgid)               │
│                                                  │
│  5. CRUSH 计算：                                 │
│     CRUSH(2.1c4, OSDMap)                        │
│       → Acting Set: [OSD 5, OSD 12, OSD 23]    │
│       → Primary: OSD 5                          │
│       → Replicas: OSD 12, OSD 23               │
└────────────────┬────────────────────────────────┘
                 │
                 ↓

第三步：发送请求到 Primary OSD
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  6. 建立连接：                                   │
│     - 目标：OSD 5 (10.0.0.15:6800)             │
│     - 协议：RADOS Protocol (TCP)                │
│                                                  │
│  7. 构造 MOSDOp 消息：                           │
│     {                                            │
│       client_id: "client.4567",                 │
│       object: "rbd_data.12345.0000000100",      │
│       pool: "rbd",                              │
│       pgid: 2.1c4,                              │
│       ops: [                                    │
│         {                                        │
│           op: CEPH_OSD_OP_WRITE,                │
│           offset: 0,                            │
│           length: 4096,                         │
│           data: [... 4KB 数据 ...]              │
│         }                                        │
│       ],                                        │
│       reqid: "client.4567.1:12345",            │
│     }                                            │
│                                                  │
│  8. 发送消息到 OSD 5                            │
└────────────────┬────────────────────────────────┘
                 │
                 │ MOSDOp
                 ↓

第四步：OSD 处理写入（Primary）
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  OSD 5 (Primary)                                │
│                                                  │
│  9. 接收并解析消息：                             │
│     - OSD::ms_dispatch()                        │
│     - 验证客户端权限                            │
│     - 检查 epoch 是否最新                       │
│                                                  │
│  10. 路由到 PG：                                 │
│     - 查找 PG 2.1c4                             │
│     - 检查 PG 状态：Active+Clean ✓              │
│                                                  │
│  11. PrimaryLogPG::do_op()                      │
│     - 分配版本号：eversion_t(1523, 789)        │
│     - 生成 PG Log Entry：                       │
│       {                                          │
│         version: (1523, 789),                   │
│         op: MODIFY,                             │
│         oid: "rbd_data.12345.0000000100",       │
│         reqid: "client.4567.1:12345",          │
│         prior_version: (1523, 788)              │
│       }                                          │
│                                                  │
│  12. 本地写入：                                  │
│     - BlueStore::queue_transaction()            │
│       • 分配 4KB 块：offset 0x8a4f3000          │
│       • aio_write(设备, 0x8a4f3000, data)       │
│       • 更新 RocksDB 元数据                     │
│       • fsync() → 持久化                        │
│     - 添加到 PG Log                             │
│                                                  │
│  13. 准备复制：                                  │
│     - 构造 MOSDRepOp 消息                       │
│     - 包含：Transaction + PG Log Entry         │
└────────────────┬────────────────────────────────┘
                 │
       ┌─────────┼─────────┐
       │         │         │
       │ MOSDRepOp        │ MOSDRepOp
       ↓         │         ↓
┌───────────┐    │    ┌───────────┐
│  OSD 12   │    │    │  OSD 23   │
│ (Replica) │    │    │ (Replica) │
└───────────┘    │    └───────────┘
       │         │         │
       ↓         │         ↓

第五步：副本写入
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  OSD 12 & OSD 23 (Replicas)                    │
│                                                  │
│  14. 接收 MOSDRepOp：                            │
│     - ReplicatedBackend::handle_message()       │
│     - 解码 Transaction                          │
│                                                  │
│  15. 本地写入：                                  │
│     - BlueStore::queue_transaction()            │
│     - 写入数据                                   │
│     - fsync()                                   │
│                                                  │
│  16. 返回确认：                                  │
│     - 发送 MOSDRepOpReply 给 Primary            │
└────────────────┬────────────────────────────────┘
                 │
       MOSDRepOpReply
                 ↓
┌─────────────────────────────────────────────────┐
│  OSD 5 (Primary - 继续)                         │
│                                                  │
│  17. 收集副本响应：                              │
│     - 等待 OSD 12: ✓                            │
│     - 等待 OSD 23: ✓                            │
│     - 所有副本已提交                            │
│                                                  │
│  18. 触发回调：                                  │
│     - on_committed()                            │
│     - 更新统计信息                              │
│                                                  │
│  19. 返回客户端：                                │
│     - 构造 MOSDOpReply                          │
│     - result: 0 (成功)                          │
│     - version: (1523, 789)                      │
└────────────────┬────────────────────────────────┘
                 │
                 │ MOSDOpReply
                 ↓

第六步：客户端接收响应
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  librados                                        │
│                                                  │
│  20. 接收 MOSDOpReply                            │
│     - 解析结果：成功                            │
│     - 返回给上层                                │
└────────────────┬────────────────────────────────┘
                 │
                 ↓
┌─────────────────────────────────────────────────┐
│  /dev/rbd0                                       │
│  - 标记 I/O 完成                                │
│  - 通知应用程序                                 │
└────────────────┬────────────────────────────────┘
                 │
                 ↓
虚拟机/应用
  write() 返回成功 ✓
```

**时间线**：
```
t=0ms    : 客户端发起写入
t=0.1ms  : 计算 CRUSH，确定目标 OSD
t=0.2ms  : 网络传输到 Primary OSD
t=1ms    : Primary 本地写入完成
t=1.1ms  : 向副本发送 MOSDRepOp
t=1.3ms  : 副本接收并开始写入
t=2ms    : 副本写入完成
t=2.1ms  : 副本返回确认
t=2.2ms  : Primary 收集所有确认
t=2.3ms  : 返回客户端
t=2.5ms  : 客户端收到成功响应
────────────────────────────────────
总延迟：~2.5ms (SSD, 3副本)
```

### 4.2 CephFS 读取流程

```
第一步：打开文件
═════════════════════════════════════════════════════════════

应用程序
   │
   │ open("/mnt/cephfs/data/file.txt", O_RDONLY)
   ↓
┌─────────────────────────────────────────────────┐
│  CephFS Client (内核模块/ceph-fuse)              │
│                                                  │
│  1. 解析路径：                                   │
│     /mnt/cephfs/data/file.txt                   │
│     → 需要遍历目录树                            │
│                                                  │
│  2. 向 MDS 请求元数据：                          │
│     - 发送 MClientRequest (LOOKUP)              │
│     - 路径："/data/file.txt"                    │
└────────────────┬────────────────────────────────┘
                 │
                 │ MClientRequest
                 ↓
┌─────────────────────────────────────────────────┐
│  MDS (Active)                                   │
│                                                  │
│  3. 查询目录：                                   │
│     - 从缓存或 RADOS 读取 /data 目录            │
│     - 查找 "file.txt" 的 inode                  │
│                                                  │
│  4. 返回文件元数据：                             │
│     {                                            │
│       inode: 12345678,                          │
│       size: 1048576,  // 1 MB                   │
│       layout: {                                 │
│         object_size: 4194304,  // 4 MB         │
│         stripe_unit: 4194304,                   │
│         stripe_count: 1,                        │
│         pool: "cephfs_data"                     │
│       },                                         │
│       mtime: "2025-01-15 10:30:00",            │
│       owner: "user1",                           │
│       permissions: 0644                         │
│     }                                            │
│                                                  │
│  5. 授予 capability（权限令牌）：               │
│     - Fr (read from file)                       │
└────────────────┬────────────────────────────────┘
                 │
                 │ MClientReply (元数据)
                 ↓
┌─────────────────────────────────────────────────┐
│  CephFS Client                                  │
│                                                  │
│  6. 缓存元数据：                                 │
│     - 本地缓存 inode 信息                       │
│     - 保存 capability                           │
│                                                  │
│  7. 返回文件描述符给应用                        │
└─────────────────────────────────────────────────┘
                 │
                 ↓
应用程序：获得 fd = 3

第二步：读取数据
═════════════════════════════════════════════════════════════

应用程序
   │
   │ read(fd, buffer, 4096)  // 读取 4KB
   ↓
┌─────────────────────────────────────────────────┐
│  CephFS Client                                  │
│                                                  │
│  8. 检查 capability：有效 ✓                     │
│                                                  │
│  9. 计算对象名：                                 │
│     - 文件偏移：0                               │
│     - 对象大小：4 MB                            │
│     - 对象编号：0                               │
│     - 对象名：                                   │
│       "12345678.00000000"                       │
│       (inode.object_number)                     │
│                                                  │
│  10. 直接通过 librados 读取：                   │
│     - pool: "cephfs_data"                       │
│     - object: "12345678.00000000"               │
│     - offset: 0                                 │
│     - length: 4096                              │
└────────────────┬────────────────────────────────┘
                 │
                 │ (绕过 MDS，直接访问 OSD)
                 ↓
┌─────────────────────────────────────────────────┐
│  librados                                        │
│                                                  │
│  11. CRUSH 计算：                                │
│     hash("12345678.00000000") → PG 3.a5        │
│     CRUSH(3.a5) → [OSD 7, OSD 14, OSD 21]     │
│                                                  │
│  12. 发送读请求到 Primary OSD：                 │
│     - 目标：OSD 7                               │
│     - MOSDOp (READ)                             │
└────────────────┬────────────────────────────────┘
                 │
                 │ MOSDOp
                 ↓
┌─────────────────────────────────────────────────┐
│  OSD 7 (Primary)                                │
│                                                  │
│  13. 定位到 PG 3.a5                             │
│                                                  │
│  14. 读取对象：                                  │
│     - BlueStore::read()                         │
│     - 查询 RocksDB 获取数据位置                 │
│     - 从设备读取 4KB 数据                       │
│     - 校验和验证 ✓                              │
│                                                  │
│  15. 返回数据：                                  │
│     - MOSDOpReply (4KB 数据)                    │
└────────────────┬────────────────────────────────┘
                 │
                 │ MOSDOpReply
                 ↓
┌─────────────────────────────────────────────────┐
│  librados → CephFS Client                       │
│  - 接收数据                                     │
│  - 拷贝到应用缓冲区                             │
└────────────────┬────────────────────────────────┘
                 │
                 ↓
应用程序
  read() 返回 4096 字节 ✓
```

**关键点**：
- ✅ **元数据查询**：通过 MDS
- ✅ **数据访问**：直接访问 OSD（绕过 MDS）
- ✅ **性能优化**：元数据缓存减少 MDS 压力

### 4.3 RGW S3 上传流程

```
第一步：客户端准备
═════════════════════════════════════════════════════════════

S3 客户端 (aws-cli/boto3)
   │
   │ PUT /my-bucket/photo.jpg
   │ Content-Length: 1048576  (1 MB)
   ↓
┌─────────────────────────────────────────────────┐
│  HTTP/HTTPS 请求                                 │
│                                                  │
│  PUT /my-bucket/photo.jpg HTTP/1.1              │
│  Host: rgw.example.com                          │
│  Authorization: AWS4-HMAC-SHA256 ...            │
│  Content-Type: image/jpeg                       │
│  Content-Length: 1048576                        │
│  x-amz-acl: private                             │
│                                                  │
│  [... 1 MB 数据 ...]                            │
└────────────────┬────────────────────────────────┘
                 │
                 │ TCP/IP
                 ↓

第二步：RGW 接收
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  radosgw 守护进程                                │
│  (HTTP 服务器：Beast/Civetweb)                  │
│                                                  │
│  1. 接收 HTTP 请求                               │
│                                                  │
│  2. 解析 S3 请求：                               │
│     - Bucket: "my-bucket"                       │
│     - Object Key: "photo.jpg"                   │
│     - ACL: private                              │
│                                                  │
│  3. 认证：                                       │
│     - 验证 AWS Signature V4                     │
│     - 查询用户信息（从 .rgw.root pool）         │
│     - 检查权限：user1 可以写入 my-bucket? ✓     │
│                                                  │
│  4. 生成对象 ID：                                │
│     - RGW 对象：                                 │
│       "my-bucket/photo.jpg"                     │
│     - RADOS 对象名：                             │
│       "default.7890.4_photo.jpg"                │
│       (zone.instance_key)                       │
└────────────────┬────────────────────────────────┘
                 │
                 ↓

第三步：写入元数据
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  RGW → librados                                  │
│                                                  │
│  5. 写入索引（.rgw.buckets.index）：            │
│     - 对象：索引对象 for "my-bucket"            │
│     - 操作：添加索引条目                        │
│     {                                            │
│       key: "photo.jpg",                         │
│       size: 1048576,                            │
│       mtime: "2025-01-15T10:30:00Z",           │
│       etag: "d41d8cd98f00b204e9800998ecf8427e", │
│       content_type: "image/jpeg",               │
│       acl: "private",                           │
│       rados_obj: "default.7890.4_photo.jpg"     │
│     }                                            │
│                                                  │
│  6. 写入元数据（.rgw.buckets.non-ec）：        │
│     - 对象：元数据对象                          │
│     - 内容：完整的对象属性和自定义元数据        │
└────────────────┬────────────────────────────────┘
                 │
                 ↓

第四步：写入数据
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  RGW → librados                                  │
│                                                  │
│  7. 写入数据（.rgw.buckets.data）：             │
│     - Pool: "default.rgw.buckets.data"          │
│     - 对象名: "default.7890.4_photo.jpg"        │
│     - 数据: [... 1 MB JPEG 数据 ...]            │
│                                                  │
│  8. librados 执行：                              │
│     - CRUSH 计算 → PG → OSDs                    │
│     - 写入流程同 RBD（前面描述的）              │
│     - 复制到 3 个 OSD                           │
└────────────────┬────────────────────────────────┘
                 │
                 ↓

第五步：返回响应
═════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────┐
│  radosgw                                         │
│                                                  │
│  9. 所有写入成功                                 │
│                                                  │
│  10. 构造 HTTP 响应：                            │
│     HTTP/1.1 200 OK                             │
│     ETag: "d41d8cd98f00b204e9800998ecf8427e"    │
│     x-amz-request-id: "tx00001234..."           │
│     Content-Length: 0                           │
│                                                  │
│  11. 发送给客户端                                │
└────────────────┬────────────────────────────────┘
                 │
                 │ HTTP 200 OK
                 ↓
S3 客户端
  上传成功 ✓
```

**RGW 数据组织**：
```
Pool 分类：

1. .rgw.root
   - 存储：区域、区域组、用户信息

2. .rgw.buckets.index
   - 存储：桶索引（对象列表）

3. .rgw.buckets.data
   - 存储：实际对象数据

4. .rgw.buckets.non-ec
   - 存储：对象元数据
```

---

## 五、CRUSH 算法与数据分布

### 5.1 CRUSH 工作原理

```
客户端写入对象 "myobject"
   │
   ▼
┌──────────────────────────────────────────────────┐
│  第一步：对象 → PG 映射                           │
│                                                   │
│  hash("myobject") = 0x7f3a2b1c                   │
│  PG = hash % pg_num                              │
│     = 0x7f3a2b1c % 1024                          │
│     = 452                                        │
│                                                   │
│  Pool ID = 2  →  PG ID = 2.452                   │
└──────────────────────────────────────────────────┘
   │
   ▼
┌──────────────────────────────────────────────────┐
│  第二步：PG → OSD 映射（CRUSH 算法）             │
│                                                   │
│  CRUSH 输入：                                     │
│  • PG ID: 2.452                                  │
│  • CRUSH Map (拓扑结构)：                        │
│    root default {                                │
│      rack rack1 {                                │
│        host host1 { osd.0, osd.1 }              │
│        host host2 { osd.2, osd.3 }              │
│      }                                            │
│      rack rack2 {                                │
│        host host3 { osd.4, osd.5 }              │
│        host host4 { osd.6, osd.7 }              │
│      }                                            │
│    }                                              │
│  • CRUSH Rule (副本规则)：                       │
│    rule replicated_rule {                        │
│      type replicated                             │
│      min_size 1                                  │
│      max_size 10                                 │
│      step take default                           │
│      step chooseleaf firstn 0 type rack         │
│      step emit                                   │
│    }                                              │
│                                                   │
│  CRUSH 输出：                                     │
│  • Acting Set: [OSD 2, OSD 5, OSD 7]           │
│  • Primary: OSD 2                                │
│  • 保证：不同 rack（机架容错）                   │
└──────────────────────────────────────────────────┘
```

**CRUSH 优势**：
1. ✅ **确定性**：相同输入总是相同输出
2. ✅ **去中心化**：客户端和 OSD 都能独立计算
3. ✅ **拓扑感知**：支持机架、数据中心等层次
4. ✅ **稳定性**：增加 OSD 时，大部分数据不移动

### 5.2 数据重平衡示例

```
场景：集群扩容，新增 OSD

原集群：8 个 OSD (0-7)
新集群：10 个 OSD (0-9)  ← 新增 OSD 8, 9

┌──────────────────────────────────────────────────┐
│  重平衡前：                                       │
│                                                   │
│  PG 2.452 → [OSD 2, OSD 5, OSD 7]               │
│  PG 2.453 → [OSD 1, OSD 4, OSD 6]               │
│  PG 2.454 → [OSD 0, OSD 3, OSD 5]               │
│  ...                                              │
└──────────────────────────────────────────────────┘
   │
   │ 添加 OSD 8, 9
   │ OSDMap epoch 1524 → 1525
   ↓
┌──────────────────────────────────────────────────┐
│  CRUSH 重新计算：                                 │
│                                                   │
│  PG 2.452 → [OSD 2, OSD 8, OSD 7]  ← OSD 5→8   │
│  PG 2.453 → [OSD 1, OSD 4, OSD 6]  ← 不变      │
│  PG 2.454 → [OSD 0, OSD 9, OSD 5]  ← OSD 3→9   │
│  ...                                              │
│                                                   │
│  只有部分 PG 受影响（~20%）                      │
└──────────────────────────────────────────────────┘
   │
   ↓
┌──────────────────────────────────────────────────┐
│  数据迁移（自动触发）：                           │
│                                                   │
│  PG 2.452:                                       │
│    OSD 5 → OSD 8 (复制所有对象)                 │
│                                                   │
│  PG 2.454:                                       │
│    OSD 3 → OSD 9 (复制所有对象)                 │
│                                                   │
│  迁移期间：                                       │
│  • 客户端 I/O 不受影响                           │
│  • 可以限流避免影响性能                          │
└──────────────────────────────────────────────────┘
```

---

## 六、集群启动与初始化

### 6.1 集群部署流程

```
第一步：部署 Monitor
═════════════════════════════════════════════════════════════

1. 在 3 台机器上安装 ceph-mon

┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│   node1      │  │   node2      │  │   node3      │
│ ceph-mon.a   │  │ ceph-mon.b   │  │ ceph-mon.c   │
└──────────────┘  └──────────────┘  └──────────────┘

2. 生成集群配置：
   $ ceph-deploy new node1 node2 node3
   
   生成文件：
   • ceph.conf           # 配置文件
   • ceph.mon.keyring    # Monitor 密钥
   • ceph.client.admin.keyring  # 管理员密钥

3. 初始化 Monitor：
   $ ceph-deploy mon create-initial
   
   每个 Monitor 启动：
   • 加载 MonMap
   • 启动 Paxos
   • 选举 Leader（Mon A）
   • 同步集群状态

第二步：部署 OSD
═════════════════════════════════════════════════════════════

4. 准备磁盘：
   $ ceph-deploy osd create node1 --data /dev/sdb
   $ ceph-deploy osd create node2 --data /dev/sdc
   $ ceph-deploy osd create node3 --data /dev/sdd
   
   每个 OSD 初始化：
   • mkfs (BlueStore 格式化)
   • 生成 OSD UUID
   • 注册到 Monitor
   • 启动 ceph-osd 进程

5. OSD 加入集群：
   
   OSD 0 启动：
   ┌─────────────────────────────────────────┐
   │ 1. 读取本地配置                          │
   │    /var/lib/ceph/osd/ceph-0/            │
   │    • fsid                                │
   │    • type (bluestore)                    │
   │    • whoami (0)                          │
   │                                          │
   │ 2. 连接 Monitor                          │
   │    • 获取最新 OSDMap                     │
   │    • 获取 CRUSH Map                      │
   │                                          │
   │ 3. 向 Monitor 报告                       │
   │    • OSD 0 is UP                         │
   │    • weight: 1.0 (1TB)                   │
   │                                          │
   │ 4. Monitor 更新 OSDMap                   │
   │    • epoch++                             │
   │    • 添加 OSD 0 到 CRUSH Map            │
   │    • 广播新 OSDMap                       │
   │                                          │
   │ 5. 创建 PG                               │
   │    • CRUSH 计算 PG 分布                 │
   │    • 激活负责的 PG                       │
   └─────────────────────────────────────────┘

第三步：创建 Pool
═════════════════════════════════════════════════════════════

6. 创建存储池：
   $ ceph osd pool create mypool 128 128
   
   Monitor 执行：
   • 分配 Pool ID: 2
   • 创建 128 个 PG (2.0 ~ 2.7f)
   • 使用 CRUSH 分配 PG 到 OSD
   • 更新 OSDMap
   
   OSD 接收通知：
   • 创建 PG 目录
   • 初始化 PG 状态机
   • 开始 Peering

第四步：验证集群
═════════════════════════════════════════════════════════════

7. 检查集群状态：
   $ ceph -s
   
   cluster:
     id:     5a9f4e58-1234-5678-9abc-def012345678
     health: HEALTH_OK
   
   services:
     mon: 3 daemons, quorum a,b,c
     mgr: node1(active)
     osd: 3 osds: 3 up, 3 in
   
   data:
     pools:   1 pools, 128 pgs
     objects: 0 objects, 0 B
     usage:   0 B used, 3 TiB / 3 TiB avail
     pgs:     128 active+clean

集群就绪！✓
```

### 6.2 客户端首次连接

```
应用程序启动
   │
   │ import rados
   │ cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
   │ cluster.connect()
   ↓
┌──────────────────────────────────────────────────┐
│  librados 初始化                                  │
│                                                   │
│  1. 读取配置文件：                                │
│     [global]                                      │
│       mon_host = 10.0.0.1,10.0.0.2,10.0.0.3      │
│                                                   │
│  2. 加载 keyring：                                │
│     /etc/ceph/ceph.client.admin.keyring          │
│     key = AQD...==                               │
│                                                   │
│  3. 连接 Monitor（轮询）：                        │
│     • 尝试 10.0.0.1:6789 → 成功                  │
│                                                   │
│  4. 认证（cephx）：                               │
│     • 发送认证请求                                │
│     • Monitor 验证密钥                            │
│     • 返回 session ticket                        │
│                                                   │
│  5. 获取集群地图：                                │
│     • 请求最新 OSDMap                             │
│     • 请求 CRUSH Map                              │
│     • 本地缓存                                    │
│                                                   │
│  6. 订阅更新：                                    │
│     • 监听 OSDMap 变化                            │
│     • 自动更新本地缓存                            │
└──────────────────────────────────────────────────┘
   │
   ↓
客户端就绪，可以执行 I/O ✓
```

---

## 七、故障处理与自愈

### 7.1 OSD 故障检测

```
场景：OSD 5 故障

┌──────────────────────────────────────────────────┐
│  故障检测机制：                                   │
│                                                   │
│  1. 心跳检测（OSD 之间）：                        │
│                                                   │
│     OSD 2 ──── heartbeat ───> OSD 5 (无响应)    │
│     OSD 7 ──── heartbeat ───> OSD 5 (无响应)    │
│                                                   │
│     超时：5 秒无响应 → 标记为疑似故障            │
│                                                   │
│  2. 向 Monitor 报告：                             │
│                                                   │
│     OSD 2 → Monitor: "OSD 5 failed since T"     │
│     OSD 7 → Monitor: "OSD 5 failed since T"     │
│                                                   │
│  3. Monitor 决策：                                │
│                                                   │
│     • 收到多个报告（多数派）                     │
│     • 确认：OSD 5 确实故障                       │
│     • 更新 OSDMap：                               │
│       - OSD 5: UP → DOWN                         │
│       - epoch: 1525 → 1526                       │
│     • 广播新 OSDMap                               │
└──────────────────────────────────────────────────┘
   │
   ↓
┌──────────────────────────────────────────────────┐
│  影响的 PG 重新映射：                             │
│                                                   │
│  旧 Acting Set:                                  │
│    PG 2.452 → [OSD 2, OSD 5, OSD 7]            │
│                       ↑                          │
│                     故障                          │
│                                                   │
│  CRUSH 重新计算：                                 │
│    PG 2.452 → [OSD 2, OSD 8, OSD 7]            │
│                       ↑                          │
│                     替代                          │
│                                                   │
│  新 Acting Set:                                  │
│    Primary: OSD 2（不变）                        │
│    Replicas: OSD 8（新）, OSD 7（不变）         │
└──────────────────────────────────────────────────┘
   │
   ↓
自动触发恢复流程
```

### 7.2 数据恢复流程

```
PG 2.452 恢复流程
═════════════════════════════════════════════════════════════

阶段 1：Peering（状态同步）
────────────────────────────────────────────────

┌──────────────────────────────────────────────────┐
│  OSD 2 (Primary)                                 │
│                                                   │
│  1. 新 OSDMap epoch 1526 到达                    │
│     • Acting Set 变化：[2, 5, 7] → [2, 8, 7]   │
│     • 触发 Peering                                │
│                                                   │
│  2. 重置 PG 状态：                                │
│     • 状态：Active → Peering                     │
│                                                   │
│  3. 收集 PG 信息（GetInfo）：                    │
│     ┌────────────────────────────────┐          │
│     │ 发送 pg_query_t 到：           │          │
│     │   • OSD 8 (新副本)             │          │
│     │   • OSD 7 (现有副本)           │          │
│     └────────────────────────────────┘          │
│                                                   │
│  4. 接收 pg_info_t：                             │
│     ┌────────────────────────────────┐          │
│     │ OSD 7 回复：                   │          │
│     │   last_update: (1525, 1234)    │          │
│     │   log_tail: (1520, 100)        │          │
│     │                                 │          │
│     │ OSD 8 回复：                   │          │
│     │   last_update: (0, 0)          │          │
│     │   (全新，无数据)                │          │
│     └────────────────────────────────┘          │
│                                                   │
│  5. 选择权威日志：                                │
│     • 对比：OSD 2 vs OSD 7                       │
│     • 权威：OSD 2 (last_update 最新)            │
│                                                   │
│  6. 生成 missing 列表：                          │
│     ┌────────────────────────────────┐          │
│     │ OSD 8 missing (全部对象):      │          │
│     │   - object_1                    │          │
│     │   - object_2                    │          │
│     │   - object_3                    │          │
│     │   - ...                         │          │
│     │   (共 1000 个对象)              │          │
│     └────────────────────────────────┘          │
│                                                   │
│  7. Peering 完成                                 │
│     • 状态：Peering → Active+Degraded           │
│     •      (降级：只有 2 副本有效)              │
└──────────────────────────────────────────────────┘

阶段 2：Recovery（数据恢复）
────────────────────────────────────────────────

┌──────────────────────────────────────────────────┐
│  OSD 2 (Primary)                                 │
│                                                   │
│  8. 启动恢复：                                    │
│     • 目标：OSD 8                                │
│     • 缺失：1000 个对象                          │
│     • 并发度：3 个对象（osd_recovery_max_active)│
│                                                   │
│  9. 恢复对象 object_1：                          │
│     ┌────────────────────────────────┐          │
│     │ a) 读取本地：                  │          │
│     │    BlueStore::read(object_1)    │          │
│     │    → 4MB 数据                   │          │
│     │                                 │          │
│     │ b) 发送到 OSD 8：               │          │
│     │    MOSDPGPush                   │          │
│     │    - object: object_1           │          │
│     │    - data: [4MB]                │          │
│     │    - version: (1525, 100)       │          │
│     └────────────────────────────────┘          │
└──────────────────────────────────────────────────┘
   │
   │ MOSDPGPush
   ↓
┌──────────────────────────────────────────────────┐
│  OSD 8 (新副本)                                  │
│                                                   │
│  10. 接收并写入：                                 │
│     • handle_push()                              │
│     • BlueStore::write(object_1, data)          │
│     • 从 missing 移除 object_1                   │
│     • 返回 MOSDPGPushReply                       │
└──────────────────────────────────────────────────┘
   │
   │ MOSDPGPushReply
   ↓
┌──────────────────────────────────────────────────┐
│  OSD 2 (Primary - 继续)                          │
│                                                   │
│  11. 更新恢复进度：                               │
│     • object_1 恢复完成 ✓                        │
│     • 剩余：999 个对象                           │
│     • 继续恢复 object_2, object_3, ...          │
│                                                   │
│  12. 重复步骤 9-11，直到所有对象恢复             │
│                                                   │
│  13. 恢复完成：                                   │
│     • missing 列表为空                           │
│     • 状态：Degraded → Clean                     │
│     • PG 现在有 3 个完整副本                     │
└──────────────────────────────────────────────────┘

恢复完成！数据完整性恢复 ✓
```

**恢复优化**：
```
1. 优先级恢复：
   • 客户端正在访问的对象优先
   • degraded（降级）对象优先

2. 智能限流：
   • 低峰时段加速恢复
   • 高峰时段降低优先级
   • 避免影响客户端 I/O

3. 增量恢复：
   • 只恢复缺失的对象
   • 利用 PG Log 确定差异
   • 无需全量扫描
```

---

## 八、总结

### 8.1 Ceph 架构特点

| 特性 | 说明 | 优势 |
|------|------|------|
| **去中心化** | CRUSH 算法，客户端直接访问 OSD | 无瓶颈，高扩展性 |
| **强一致性** | Paxos (Monitor) + PG Log (OSD) | 数据可靠 |
| **自我管理** | 自动复制、恢复、重平衡 | 降低运维成本 |
| **统一存储** | 一个集群三种接口 | 简化架构 |
| **容错能力** | 副本/EC，自动故障检测 | 高可用 |

### 8.2 性能数据（参考）

```
延迟（SSD，3 副本）：
  - 4KB 随机写：~2ms
  - 4KB 随机读：~0.5ms
  - 顺序写：~200MB/s (单 OSD)
  - 顺序读：~500MB/s (单 OSD)

IOPS（SSD，3 副本）：
  - 4KB 随机写：~500 IOPS (单 OSD)
  - 4KB 随机读：~2000 IOPS (单 OSD)

扩展性：
  - 单集群：数千 OSD
  - 容量：EB 级别
  - 客户端：数万并发连接
```

### 8.3 最佳实践

**集群设计**：
- ✅ Monitor：3 或 5 个（奇数）
- ✅ OSD：每个节点 10-12 个磁盘
- ✅ 网络：10Gb/s 或以上（推荐 25Gb/s）
- ✅ PG 数量：100-200 PG per OSD

**性能调优**：
- ✅ 使用 SSD 做 BlueStore WAL
- ✅ 启用 mClock 调度器
- ✅ 调整恢复参数避免影响客户端
- ✅ 使用 Cache Tier 加速（可选）

**可靠性**：
- ✅ 至少 3 副本（或 4+2 EC）
- ✅ 定期 Deep Scrub
- ✅ 监控 HEALTH_WARN
- ✅ 定期测试故障恢复

---

## 🎓 学习建议

1. **动手实践**：搭建小型测试集群（3 节点）
2. **阅读源码**：从客户端库 librados 开始
3. **监控观察**：使用 `ceph -w` 观察集群行为
4. **故障模拟**：kill OSD 进程，观察恢复
5. **性能测试**：使用 rados bench、fio 等工具

---

Ceph 是一个**生产级的分布式存储系统**，理解其架构和交互流程是掌握分布式存储的关键！🚀

